{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression: A Visual Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a conceptual and practical introduction to Gaussian Process Regression (GPR). We will explore what GPR is, how it compares to Linear Regression, and when to use it.\n\n**Key goals of this notebook:**\n1. Understand the fundamental difference between Linear Regression and GPR.\n2. Fit both models to nonlinear data and compare their performance.\n3. Visualize GPR's uncertainty estimates.\n4. Discuss the use cases and limitations of GPR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Core Idea: Modeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression** and **Gaussian Process Regression** are both supervised learning models used for regression tasks. However, they approach the problem from fundamentally different perspectives.\n\n- **Linear Regression** assumes a **parametric** form for the function. It fits a straight line (or a hyperplane) to the data by finding the optimal parameters (slope and intercept).\n  - **Model:** $y = X\beta + \epsilon$\n\n- **Gaussian Process Regression** is a **non-parametric** model. Instead of fitting a single function, it defines a **distribution over functions**. It uses a kernel to model the similarity between data points, allowing it to capture complex, nonlinear relationships.\n  - **Model:** $f(x) \sim \mathcal{GP}(m(x), k(x, x'))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A Practical Example: Fitting Nonlinear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some nonlinear data from a sine function and see how each model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\n# Generate nonlinear data\nnp.random.seed(0)\nX = np.linspace(0, 10, 100).reshape(-1, 1)\ny_true = np.sin(X).ravel()\ny = y_true + np.random.normal(0, 0.2, X.shape[0])\n\n# Plot the data\nplt.figure(figsize=(10, 5))\nplt.plot(X, y_true, label='True function $sin(x)$', color='black', linestyle='--')\nplt.scatter(X, y, label='Noisy observations', color='blue', alpha=0.6)\nplt.title('Nonlinear Data from a Sine Function')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Linear Regression: A Poor Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, Linear Regression fails to capture the nonlinear pattern in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\nlr.fit(X, y)\ny_lr_pred = lr.predict(X)\n\nplt.figure(figsize=(10, 5))\nplt.plot(X, y_true, label='True function $sin(x)$', color='black', linestyle='--')\nplt.scatter(X, y, label='Noisy observations', color='blue', alpha=0.6)\nplt.plot(X, y_lr_pred, label='Linear Regression', color='red', linewidth=2)\nplt.title('Linear Regression Fit')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Gaussian Process Regression: A Flexible Fit with Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPR, on the other hand, provides a much more flexible fit. It not only captures the underlying sine function but also provides a **confidence interval** (the shaded area), quantifying its uncertainty at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\ngpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=0.2**2)\ngpr.fit(X, y)\ny_gp_pred, y_gp_std = gpr.predict(X, return_std=True)\n\nplt.figure(figsize=(10, 5))\nplt.plot(X, y_true, label='True function $sin(x)$', color='black', linestyle='--')\nplt.scatter(X, y, label='Noisy observations', color='blue', alpha=0.6)\nplt.plot(X, y_gp_pred, label='GP Mean Prediction', color='green', linewidth=2)\nplt.fill_between(X.squeeze(), y_gp_pred - 2*y_gp_std, y_gp_pred + 2*y_gp_std, color='lightgreen', alpha=0.5, label='GP 95% Confidence Interval')\nplt.title('Gaussian Process Regression Fit')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. When to Use Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPR is a powerful tool, but it's not always the right choice. Hereâ€™s a summary of its strengths and weaknesses:\n\n**Use GPR when:**\n- You expect a **smooth, nonlinear relationship** in your data.\n- **Quantifying uncertainty** is important for your application.\n- Your dataset is **not excessively large** (GPR scales poorly, typically O(n^3)).\n\n**Avoid GPR when:**\n- Your dataset is **very large** (thousands of data points or more).\n- The underlying function is **not smooth** or has sharp discontinuities.\n- You have a **high-dimensional** input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Visualizing the Prior and Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build intuition, let's visualize the GPR model's 'state of mind' before and after seeing data.\n\n### The Prior: Before Seeing Data\nBefore we fit the model, the GP prior is a distribution over all possible smooth functions. The mean is typically zero, and the uncertainty is high everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_prior = GaussianProcessRegressor(kernel=kernel, alpha=1e-10)\ny_prior_mean, y_prior_std = gpr_prior.predict(X, return_std=True)\n\nplt.figure(figsize=(10, 5))\nplt.plot(X, y_prior_mean, label='Prior Mean', color='black', linestyle='--')\nplt.fill_between(X.squeeze(), y_prior_mean - 2*y_prior_std, y_prior_mean + 2*y_prior_std, color='lightgray', alpha=0.7, label='Prior 95% CI')\nplt.title('GP Prior: High Uncertainty Everywhere')\nplt.xlabel('X')\nplt.ylabel('f(x)')\nplt.legend()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Posterior: After Seeing Data\nOnce we provide data, the GP updates its beliefs. The posterior distribution is much more constrained: the mean prediction passes through the data points, and the uncertainty shrinks in their vicinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_obs = np.array([[2], [5], [8]])\ny_obs = np.sin(X_obs).ravel()\n\ngpr_post = GaussianProcessRegressor(kernel=kernel, alpha=1e-10)\ngpr_post.fit(X_obs, y_obs)\ny_post_mean, y_post_std = gpr_post.predict(X, return_std=True)\n\nplt.figure(figsize=(10, 5))\nplt.plot(X, y_post_mean, label='Posterior Mean', color='blue', linewidth=2)\nplt.fill_between(X.squeeze(), y_post_mean - 2*y_post_std, y_post_mean + 2*y_post_std, color='lightblue', alpha=0.5, label='Posterior 95% CI')\nplt.scatter(X_obs, y_obs, color='red', label='Observed Data', zorder=10, s=100)\nplt.title('GP Posterior: Uncertainty Shrinks Near Data')\nplt.xlabel('X')\nplt.ylabel('f(x)')\nplt.legend()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: Mathematical Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian Process is defined by a **mean function** $m(x)$ and a **covariance function** (or kernel) $k(x, x')$.\n\n- The **kernel** is the heart of the GP. It encodes our assumptions about the function's smoothness and other properties. The RBF kernel used here is a common choice:\n  $$ k(x, x') = \sigma^2 \exp\left(-\frac{(x-x')^2}{2\ell^2}\right) $$\n  where $\sigma^2$ is the variance and $\ell$ is the length scale.\n\n- The **posterior predictive distribution** for a new point $x_*$ is also a Gaussian, with mean and variance given by:\n  $$ \mu_* = K(X_*, X)[K(X, X) + \sigma_n^2 I]^{-1} y $$\n  $$ \Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma_n^2 I]^{-1}K(X, X_*) $$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}